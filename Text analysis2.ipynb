{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Импортируем `pymorphy2` (эта чудная библиотека умеет много чего, в том числе ставить слова в начальную форму и определять часть речи, но работает медленно). `Counter`, `pandas` и регулярки нам также пригодятся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Открываем текст и делим его на слова с помощью регулярных выражений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open('The three ages2.txt').read()\n",
    "words=re.split(r'\\W+', words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Создаем словарь, в котором ключами будут слова из текста, а значениями – количество их повторений. В цикле приводим слова в начальную форму, избавляемся от случайных пустых строк и служебных частей речи "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "for i in words:\n",
    "    m=pymorphy2.MorphAnalyzer().parse(i)[0]\n",
    "    if (i and m.tag.POS not in ['PREP', 'CONJ', 'PRCL', 'INTJ', 'NPRO']):\n",
    "        vocab.append(m.normal_form)\n",
    "vocab=dict(Counter(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Создаем `DataFrame` из .csv с частотным словарем русского языка, складываем значения для повторяющихся слов (в этом словаре записаны, например, несколько \"а\" как разные части речи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=pd.read_csv('freqrnc2011.csv', sep='\\t', index_col='Lemma')\n",
    "freq=freq.groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Для слов из нашего текста, которые в словаре есть, частоту приводим к общей с .csv единице измерения (слов на миллион) и делим на значение из частотного словаря, убирая из топа слова, которые просто чаще всего в русском языке встречаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in vocab:\n",
    "    i=re.sub(r'ё', 'е', j)\n",
    "    if i in freq.index:\n",
    "        vocab[j]*=1000000/len(words)\n",
    "        vocab[j]=round(vocab[j]/freq.loc[i, 'Freq(ipm)'], 2)\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Сортируем наш словарь (покажем первые 15 строчки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('клипер', 19607.84)\n",
      "('флегматично', 7843.14)\n",
      "('чертовка', 7843.14)\n",
      "('шпиц', 6535.95)\n",
      "('поторапливаться', 6535.95)\n",
      "('джигит', 5602.24)\n",
      "('геройски', 4357.3)\n",
      "('растяпа', 4357.3)\n",
      "('педант', 3565.06)\n",
      "('пекинский', 3267.97)\n",
      "('мичман', 3179.65)\n",
      "('господский', 3016.59)\n",
      "('гашиш', 3016.59)\n",
      "('древние', 2614.38)\n",
      "('консул', 2450.98)\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(list(vocab.items()), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
